{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb4709e-e146-48b3-853f-89a38c22bd37",
   "metadata": {},
   "source": [
    "# U-net architecture and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f2725-53b4-466a-aefc-d2acb6c1f25d",
   "metadata": {},
   "source": [
    "## Select model type, data, number of epochs and training step multiplier (user input required)\n",
    "### Select the model\n",
    "If pretrained model is selected, use_onehot_masks is forced to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958151a-729d-4383-86db-37c4eef2b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = int(input(\"Select U-net type: [0] = U-net from Scratch, [1] = Down Convolution with MobileNetV2, [2] = Down Convolution with VGG16:\"))\n",
    "\n",
    "if selected_model == 0:\n",
    "    pretrained_model = 'U-net_from_scratch'\n",
    "    use_onehot_masks = True\n",
    "    print('Running U-net from scratch without a pretrained model')\n",
    "elif selected_model == 1:\n",
    "    pretrained_model = 'MobileNetV2'\n",
    "    use_onehot_masks = False\n",
    "    print('Running U-net with', pretrained_model, 'as pretrained model in the down convolution path')\n",
    "elif selected_model == 2:\n",
    "    pretrained_model = 'VGG16'\n",
    "    use_onehot_masks = False\n",
    "    print('Running U-net with', pretrained_model, 'as pretrained model in the down convolution path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43d00a-d386-4f40-a148-96aba1837605",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select cell type for model input (only cort, shsy5y or astro or all 3 types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468699d1-e8a7-43fb-85bf-e962a4cda431",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types = [\"astro\", \"shsy5y\", \"cort\"]\n",
    "\n",
    "which_cell_id = int(input(\"Select mask type: [0] = All, [1] = astro, [2] = shsy5y, [3] = cort, 0, 1, 2 or 3:\")) # 0 = ALL; 1 id starts with first unique cell_type\n",
    "\n",
    "if which_cell_id == 0:\n",
    "    which_cell = 'all'\n",
    "    print('Running U-net with', which_cell, 'data')\n",
    "else:    \n",
    "    which_cell = cell_types[which_cell_id-1]\n",
    "    \n",
    "    print('Running U-net with only', which_cell, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e8b2d-5fe2-4cba-bebe-d94e5c711e03",
   "metadata": {},
   "source": [
    "### Select number of training epochs and training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b2764-a508-44ab-a847-94c74e087516",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs            = int(input(\"Number of epochs [integer > 0 or -1 for default of 25]:\"))\n",
    "TRAINING_STEPS_MULT = int(input(\"Training step multiplier [integer > 0 or -1 for default of 2]:\"))\n",
    "\n",
    "if n_epochs < 1:\n",
    "    n_epochs = 25\n",
    "\n",
    "if TRAINING_STEPS_MULT < 1:\n",
    "    TRAINING_STEPS_MULT = 2\n",
    "    \n",
    "print(f\"Train model with {n_epochs} epochs and {TRAINING_STEPS_MULT} fold training steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f37931-1ca9-42f7-9191-333ed905302d",
   "metadata": {},
   "source": [
    "## Imports and preparation\n",
    "### Set the desired image size and number of channels of the segmentation mask (single channel versus one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd328cf-25fc-4689-80b9-a193bb016660",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_onehot_masks is True:\n",
    "    CHANNELS_MASK = 3\n",
    "else:\n",
    "    CHANNELS_MASK = 1\n",
    "\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "\n",
    "with open('image_modeling.py', 'r') as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "lines.insert(0, f\"HEIGHT = {HEIGHT}\\n\")\n",
    "lines.insert(0, f\"WIDTH = {WIDTH}\\n\")\n",
    "lines.insert(0, f\"CHANNELS_MASK = {CHANNELS_MASK}\\n\")\n",
    "lines.insert(0, \"\\n\")\n",
    "\n",
    "with open('image_modeling_import.py', 'w') as outfile:\n",
    "    outfile.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee530c24-3322-41ba-8b92-daa181cf670e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fdf22-df4a-4794-bb34-55d0bcb03cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import image_modeling_import   # import image_modeling_import.py file\n",
    "import u_net_functions as unf\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "import segmentation_models as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90500784-3a0f-441f-9ea9-393ae575e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/\n",
    "\n",
    "# Check for Tensorflow version\n",
    "print(tf.__version__)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "\n",
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49875a71-7a6c-4033-a805-73997f1042e2",
   "metadata": {},
   "source": [
    "#### Load csv files depending on the mask's number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4bdee-d50d-473a-a26d-98428e59551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if which_cell_id == 0:\n",
    "    if use_onehot_masks is True:\n",
    "        TRAIN_PATH = 'CSVs/cells_train_oh.csv'\n",
    "        EVAL_PATH  = 'CSVs/cells_eval_oh.csv'\n",
    "        TEST_PATH  = 'CSVs/cells_test_oh.csv'\n",
    "    \n",
    "    else:\n",
    "        TRAIN_PATH = 'CSVs/cells_train.csv'\n",
    "        EVAL_PATH  = 'CSVs/cells_eval.csv'\n",
    "        TEST_PATH  = 'CSVs/cells_test.csv'\n",
    "else:\n",
    "    if use_onehot_masks is True:\n",
    "        TRAIN_PATH = 'CSVs/cells_train_oh_{}.csv'.format(which_cell)\n",
    "        EVAL_PATH  = 'CSVs/cells_eval_oh_{}.csv'.format(which_cell)\n",
    "        TEST_PATH  = 'CSVs/cells_test_oh_{}.csv'.format(which_cell)\n",
    "    \n",
    "    else:\n",
    "        TRAIN_PATH = 'CSVs/cells_train_{}.csv'.format(which_cell)\n",
    "        EVAL_PATH  = 'CSVs/cells_eval_{}.csv'.format(which_cell)\n",
    "        TEST_PATH  = 'CSVs/cells_test_{}.csv'.format(which_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58437157-178b-4505-9809-76ab49cb38a4",
   "metadata": {},
   "source": [
    "#### Import variables from image_modelling.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef3272-672e-40bf-9d61-7b50b1262d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = image_modeling_import.HEIGHT\n",
    "WIDTH = image_modeling_import.WIDTH\n",
    "BATCH_SIZE = image_modeling_import.BATCH_SIZE\n",
    "\n",
    "# Training size depends on number of training samples\n",
    "if which_cell_id == 0:\n",
    "    TRAINING_SIZE = len(pd.read_csv('CSVs/cells_train.csv'))\n",
    "    TRAINING_STEPS = int(TRAINING_SIZE) // BATCH_SIZE\n",
    "else:\n",
    "    TRAINING_SIZE = len(pd.read_csv('CSVs/cells_train_{}.csv'.format(which_cell)))\n",
    "    TRAINING_STEPS = int(TRAINING_SIZE) // BATCH_SIZE\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a1baf-eb48-43a4-9fde-5399793c8128",
   "metadata": {},
   "source": [
    "### Functions to display images, masks and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69627367-c99b-41ad-a3f0-870893834f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            model.predict(image)\n",
    "            unf.display([image[0], mask[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        unf.display([sample_image, sample_mask,\n",
    "                 unf.create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9117b2ad-da02-4c23-af69-917309d68e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c439af-8d47-4564-a625-2506ea1b6889",
   "metadata": {},
   "source": [
    "### Function for training and evaluation (kept in this notebook for easier modification of number of epochs etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c49704-6192-4e96-9e0a-4f6e2d073dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for training and evaluation\n",
    "def train_and_evaluate(model, batch_size, n_epochs = 25, reinitialize = False):\n",
    "    \n",
    "    if use_onehot_masks is True:\n",
    "        model.compile(\n",
    "            optimizer=\"adam\", \n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    else:\n",
    "        # loss for single channel mask\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy']) # metrics = sm.metrics.IOUScore(threshold=0.5)\n",
    "    \n",
    "    if reinitialize is True:\n",
    "        unf.reinitialize_weights(model)\n",
    "    \n",
    "    dataset = image_modeling_import.load_dataset_seg(TRAIN_PATH, batch_size, training = True)\n",
    "    eval_dataset = image_modeling_import.load_dataset_seg(EVAL_PATH, batch_size, training=False)\n",
    "    \n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    results = model.fit(\n",
    "                        dataset, \n",
    "                        validation_data=eval_dataset,\n",
    "                        steps_per_epoch=TRAINING_STEPS * TRAINING_STEPS_MULT,\n",
    "                        epochs=n_epochs,\n",
    "                        callbacks=[DisplayCallback()]\n",
    "    )\n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0681ff1-5a98-47b1-84f8-7631962e7ba3",
   "metadata": {},
   "source": [
    "## Compile the U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb7466-16af-4241-87e2-a02dd32a9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the helper function for defining the layers for the model, given the input image size\n",
    "if use_onehot_masks:\n",
    "    OUTPUT_CLASSES = 3\n",
    "else:\n",
    "    OUTPUT_CLASSES = 2\n",
    "\n",
    "\n",
    "if pretrained_model == 'U-net_from_scratch':\n",
    "    model = unf.UNetCompiled(input_size=(128,128,3), n_filters=32, n_classes= OUTPUT_CLASSES)\n",
    "\n",
    "elif pretrained_model == 'VGG16':\n",
    "    model = unf.unet_model(OUTPUT_CLASSES, 'VGG16')\n",
    "    \n",
    "elif pretrained_model == 'MobileNetV2':\n",
    "    model = unf.unet_model(OUTPUT_CLASSES, 'MobileNetV2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cb3ae-2a81-434a-a104-d5c59e9df379",
   "metadata": {},
   "source": [
    "## Example prediction prior to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db05ada-83bd-4202-969f-02e5744e8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = image_modeling_import.load_dataset_seg(TRAIN_PATH, 2, training = False)\n",
    "\n",
    "for images, masks in dataset.take(2):\n",
    "    sample_image, sample_mask = images[1], masks[1]\n",
    "    #display([sample_image, sample_mask])\n",
    "\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df85bc-cfd3-4b1c-b688-136437b50675",
   "metadata": {},
   "source": [
    "## Train the U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73a861-2cc5-4f63-a287-3a6dd6672b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, results = train_and_evaluate(model, BATCH_SIZE, n_epochs = n_epochs, reinitialize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19011b9c-c6f4-4ec4-943c-5191b3b89bc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Store trained model for later use (more training epochs or prediction on the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c59d5e-d1bd-4236-831a-3e4065cfd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "\n",
    "model_name = f\"{which_cell}_{pretrained_model}_steps_{TRAINING_STEPS * TRAINING_STEPS_MULT}_epochs_{n_epochs}\"\n",
    "\n",
    "trained_model.save(f\"saved_model/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7f2c8-d4e9-438f-b053-ff16dde19600",
   "metadata": {},
   "source": [
    "## Loss/accuracy per epoch figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5d0ef-38a0-4adb-9a5d-428536a01930",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(20, 5))\n",
    "axis[0].plot(results.history[\"loss\"], color='r', label = 'train loss')\n",
    "axis[0].plot(results.history[\"val_loss\"], color='b', label = 'val loss')\n",
    "axis[0].set_title('Loss Comparison')\n",
    "axis[0].legend()\n",
    "axis[1].plot(results.history[\"accuracy\"], color='r', label = 'train accuracy')\n",
    "axis[1].plot(results.history[\"val_accuracy\"], color='b', label = 'val accuracy')\n",
    "axis[1].set_title('Accuracy Comparison')\n",
    "axis[1].legend()\n",
    "\n",
    "\n",
    "!mkdir -p training_history\n",
    "\n",
    "plt.savefig(f'training_history/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e563c-a679-46e7-95fd-a54046e7644d",
   "metadata": {},
   "source": [
    "## Load the model and keep on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14ea78-56c8-49d1-a56a-9d13ab696b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = tf.keras.models.load_model(f'saved_model/{model_name}')\n",
    "\n",
    "trained_model, results = train_and_evaluate(model, BATCH_SIZE, n_epochs = 2)\n",
    "\n",
    "model_name = model_name +f'+{n_epochs}'\n",
    "trained_model.save(f'saved_model/{model_name}')\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825ec95-4807-44f5-9507-134c5d8042da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(20, 5))\n",
    "axis[0].plot(results.history[\"loss\"], color='r', label = 'train loss')\n",
    "axis[0].plot(results.history[\"val_loss\"], color='b', label = 'val loss')\n",
    "axis[0].set_title('Loss Comparison')\n",
    "axis[0].legend()\n",
    "axis[1].plot(results.history[\"accuracy\"], color='r', label = 'train accuracy')\n",
    "axis[1].plot(results.history[\"val_accuracy\"], color='b', label = 'val accuracy')\n",
    "axis[1].set_title('Accuracy Comparison')\n",
    "axis[1].legend()\n",
    "\n",
    "plt.savefig(f'training_history/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ef805-c11e-4c5b-a102-38c9f5ff9434",
   "metadata": {},
   "source": [
    "## Show example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f06b994-be48-40dd-be97-e5742be12e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = image_modeling_import.load_dataset_seg(TRAIN_PATH, 32, training = False)\n",
    "\n",
    "for images, masks in dataset.take(32):\n",
    "    sample_image, sample_mask = images[0], masks[0]\n",
    "\n",
    "    show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe79c5-63ba-420e-8c1f-2fb79f70e213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7bfba5-0cd3-4e5f-8f3a-bf269293332d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
